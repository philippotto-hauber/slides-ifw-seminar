\documentclass[xcolor=svgnames, 10pt, aspectratio=169]{beamer}

% packages 
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{lmodern}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{natbib}
\usepackage{bibentry}
\usepackage{tcolorbox}
\usepackage{caption}
\usepackage{setspace}
\usepackage{adjustbox}
\usepackage{listings}
\usepackage{parcolumns}
\usepackage{textcomp} 
\usepackage{algorithm}
\usepackage{algpseudocode}

\lstset{ %
language=Matlab,
numbers=none,           % where to put the line-numbers
numberstyle=\tiny,      % the size of the fonts that are used for the line-numbers
basicstyle=\footnotesize,    % the size of the fonts that are used for the line-numbers
escapeinside={(*@}{@*)}
}

% don't display Figure or Table before caption
\captionsetup[figure]{labelformat=empty}% redefines the caption setup of the figures environment in the beamer class.

% Algorithms
\renewcommand{\thealgorithm}{2.\arabic{algorithm}}
\setcounter{algorithm}{0}

% space, not comma between author and year
\setcitestyle{aysep={}}

% Fonts
\usepackage{mathpazo}

% Template Setup
\setbeamertemplate{frametitle}{\vskip+3pt \insertframetitle \vskip+3pt \usebeamerfont{framesubtitle}\usebeamercolor[fg]{framesubtitle}\insertframesubtitle \vskip-1pt \noindent \centerline{\rule{\textwidth}{0.4pt}}}
\setbeamerfont{frametitle}{series=\bfseries, size=\Large}
\setbeamerfont{framesubtitle}{series=\bfseries, size=\normalsize}
\setbeamercolor{frametitle}{fg=myblue}
\setbeamercolor{framesubtitle}{fg=black}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{date}{fg=myblue}

% Colors
\colorlet{myblue}{MidnightBlue!60!black!82}
\colorlet{myred}{Red!60!black!82}
\definecolor{amber}{rgb}{1.0, 0.75, 0.0}

% tikz 
\usepackage{tikz}
\usetikzlibrary{spy,calc,arrows,decorations.pathreplacing}

% Set up the `tcolorboxes'
\tcbset{
    noparskip,
    colback=Silver!8, 
    colframe=myblue, 
    boxrule=0.3pt,
    coltext=black,
    coltitle=white, 
    fonttitle=\large\bfseries
    }
    
% Links
\setbeamercolor{button}{bg=myblue,fg=white}    

% Itemize Item
\setbeamercolor{itemize item}{fg=myblue, bg=white}
\setbeamercolor{itemize subitem}{fg=myblue, bg=white}
\setbeamercolor{itemize subsubitem}{fg=myblue, bg=white}

\setbeamertemplate{itemize item}{\Large \raise-1pt \hbox{\textbullet}} 
\setbeamertemplate{itemize subitem}{\large \raise-1pt \hbox{\textbullet}} 
\setbeamertemplate{itemize subsubitem}{\small \raise-1pt \hbox{\textbullet}} 

% Remove navigation bar
\beamertemplatenavigationsymbolsempty

% Include frame number
%\setbeamertemplate{footline}[frame number]

% start full screen
%\hypersetup{pdfpagemode=FullScreen}

% At the beginning of every section
\AtBeginSection[]
{
  \begin{frame}
    \vfill 
    \centering
    {\color{myblue} \bfseries \huge \insertsectionhead}
    \vfill 
 
  \end{frame}
}

% Macros


% Statistics
\newcommand\pN{\mathcal{N}} % Normal distribution
\newcommand\iidN{\overset{iid}{\sim}\mathcal{N}} % iid Normal
\newcommand\E{\mathbb{E}} % Expectations Operator
\newcommand\transpose[1]{{#1}^{\sf{T}}}

% Title Page
\title{ \bfseries \LARGE Precision-based sampling with missing observations}
\date{\vfill \bfseries Kiel Institute Research Seminar \\[4pt] March 22\textsuperscript{nd}, 2022}	

\author[shortname]{
                   Philipp Hauber\inst{1,2}
                }  

\institute[shortinst]{
                      \inst{1} JMU Würzburg \\
                      \inst{2} Kiel Institute for the World Economy (IfW)                        
                     }
 
% Body of the presentation
\begin{document}

%\setbeamertemplate{footline}{} % don't display slide numbers!
\begin{frame}
	\vskip+5.5pt
    \centerline{\rule{1.1\textwidth}{0.4pt}}
	
    \maketitle

    \centerline{\rule{1.1\textwidth}{0.4pt}}
\end{frame}   

\begin{frame}{This presentation}{}
    Mash-up of two papers in my dissertation!\\~\\

    

    Method: 
    \vspace{0.1cm}

    Hauber, P and C. Schumacher (2021). \textit{Precision-based sampling with missing observations: A factor model application}, \textbf{Bundesbank Discussion Paper 11/2021}.
    \\~\\
    Application:  
    \vspace{0.1cm}

    Hauber, P. (2021) \textit{How useful is external information from professional forecasters? Conditional forecasts in large factor models} \medskip
\end{frame}

\begin{frame}{Precision-based sampling}{Motivation}

Essential task in the Bayesian estimation of state space models: drawing from $p(\boldsymbol{\eta} | \mathbf{y}, \Theta)$ where $\boldsymbol{\eta}$ is an unobserved component, $\mathbf{y}$ is data and $\Theta$ parameters \\~\\

Precision-based samplers (\citealp[][\scriptsize \textbf{IJMMNO}\normalsize]{chanjelizakov_2009}; \citealp[][\scriptsize \textbf{JEcmtrics}\normalsize]{mccausland_2012}) exploit the fact the precision matrix of $\boldsymbol{\eta}$ is banded in many macroeconomic application $\rightarrow$ alternative to simulation smoothers that rely on the Kalman filter \\~\\

Applications in macroeconomics (with complete data) include models of trend inflation (\citealp[][\scriptsize \textbf{JBES}\normalsize]{chankooppotter_jae2013}), time-varying Bayesian vector autoregressions \citep[][\scriptsize \textbf{JBES}\normalsize]{chan_2020jbes} and factor models (\citealp[][\scriptsize \textbf{JAE}\normalsize]{kaufmannschumacher_jae2017})\\~\\

Missing observations arise frequently in macroeconomic applications/datasets: different starting dates, different release patterns ("ragged edge"), outliers or mixed frequencies\\~\\

In our paper, we propose a precision-sampler that can handle (most of) these applications! 

\end{frame}

\begin{frame}{Precision-based sampling}{Simple example}
AR(1) process: $\eta_t = \phi \eta_{t-1} + u_t; \, u_t \sim \mathcal{N}(0, \sigma^2)$ \\~\\

Stacking the observations over $t = 1, \dots, T$ yields 
$$
\mathbf{H} \boldsymbol{\eta} = \mathbf{u}, \: \text{where} \: \mathbf{u} \sim \mathcal{N}(0, \mathbf{I}_T \sigma^2) \: \text{and} \:
\mathbf{H} = 
\left[
\begin{smallmatrix}
1 &  &  &  &  \\
-\phi & 1 &  &  &  \\
 & -\phi & 1 &  &  \\
 &  & & \ddots & \ddots &  & \\
 &  &  &  & -\phi & 1
\end{smallmatrix}
\right]
$$ \medskip


$\boldsymbol{\eta}$ is Normal with mean $\mathbf{0}_T$ and covariance matrix $\Sigma = \mathbf{H}^{-1} \, \mathbf{I}_T\sigma^{2} \, \transpose{\mathbf{H}^{-1}}$ \\~\\

corresponding \textit{precision matrix} is given by $\mathbf{Q} = \Sigma^{-1} = \transpose{\mathbf{H}} \, \mathbf{I}_T\sigma^{-2} \, \mathbf{H}$
\end{frame}

\begin{frame}{Precision-based sampling with missing observations}{Covariance and precision matrix of $\boldsymbol{\eta}$}

    Properties of the multivariate $\mathcal{N}$: 

    \begin{itemize}
        \item $\Sigma_{ij} = 0$ $\Longrightarrow$ independence of $\eta_i$ and $\eta_j$ (in the example: $\:Cov(\eta_i, \eta_j) \propto \phi^{|i-j|}$)
        \item $\mathbf{Q}_{ij} = 0$ $\Longrightarrow$ \textbf{conditional} independence of $\eta_i$ and $\eta_j$ \\~\\
    \end{itemize}    

    \begin{adjustbox}{minipage=0.7\textwidth,center}
        \begin{figure}
            \includegraphics[scale = 0.65]{fig_Sigma_Q.png}  \vspace{0.1cm} \\
        \end{figure}
            \setstretch{0.1}
            {\tiny \textbf{Notes}: The blue dots indicate the non-zero entries in the covariance matrix $\Sigma$ and precision matrix $\mathbf{Q}$ of an AR(1) process for $T=20$ observations. The former is a dense matrix while the latter is sparse and banded with lower and upper bandwidth equal to 1.}\par
    \end{adjustbox}
\end{frame}

\begin{frame}[fragile]{Precision-based sampling}{Computational advantages of banded precision matrices}

    Solving linear systems of the form $Ux = b$ where $U$ is an $n \times n$ upper-triangular matrix takes $n^2$ flops (left); when U has bandwidth $p$ the solution can be obtained in $2 n p$ flops (right):

    \begin{columns}
        \column{0.5\textwidth}
        \begin{lstlisting}
    % solution to Ux = b
    % U has maximal bandwidth
    for i = n:-1:1
        x(i) = b(i)/U(i,i)
        for j = 1:i-1
            b(j) = b(j) - U(j,i)x(i)
        end
    end
        \end{lstlisting}
        %\hfill
        \column{0.5\textwidth}
        \begin{lstlisting}
% solution to Ux = b
% U has bandwidth p            
for i = n:-1:1
    x(i) = b(i)/U(i,i)
    for j = (*@\textcolor{myred}{\,\textbf{max}\,\{1\,,\,i\,\textendash\,p\,\}}@*):i-1
        b(j) = b(j) - U(j,i)x(i)
    end
end
        \end{lstlisting}
    \end{columns}
    \vspace{0.3cm}

    Even larger gains for matrix factorisations, e.g. Cholesky ($Q = L\transpose{L}$) $\Longrightarrow$ linear instead of cubic costs! \\~\\

    $L$ "inherits" the bandwidth of $Q$ \citep[][Theorem 4.3.1]{golubvanloan2013}
\end{frame}

\begin{frame}{Precision-based sampling}{Factor model}
    To fix ideas, consider the following factor model:
    \begin{align*}
        \mathbf{y}_t &= \lambda \boldsymbol{\eta}_t + \mathbf{e}_t ;\: \mathbf{e}_t \sim \mathcal{N}(0, \Sigma_e)\\
        \boldsymbol{\eta}_t &= \phi^{\eta} \boldsymbol{\eta}_{t-1} + \mathbf{u}_t;\: \mathbf{u}_t \sim \mathcal{N}(0, \Sigma_u)
    \end{align*}

    where $\mathbf{y}_t$ is an $N\times 1$ vector of data, $\boldsymbol{\eta}_t$ is an $R\times 1$ vector of unobserved factors and $\Sigma_e = diag([\sigma^2_1, \cdots, \sigma^2_N]))$ a diagonal matrix \\~\\

    Bayesian estimation of the model is done via a Gibbs Sampler which sequentially draws from \vspace{0.2cm}
    \begin{itemize}
        \item the conditional distribution of factors given data and parameters: $p(\boldsymbol{\eta} | \mathbf{y}, \Theta)$ 
        \item the conditional distribution of parameters given data and factors:$p(\Theta | \boldsymbol{\eta}, \mathbf{y})$
    \end{itemize}

    %where $\Theta = [\lambda, \phi^e, \phi^{\eta}, \Sigma_u, \sigma_1^2, \cdots, \sigma_N^2]$

\end{frame}

\begin{frame}[fragile]{Precision-based sampling}{Drawing from $p(\boldsymbol{\eta} | \mathbf{y}, \Theta)$}
    \label{precsampl_draw_eta}
    Joint distribution of factors $\boldsymbol{\eta} = \transpose{[\transpose{\boldsymbol{\eta}_1}, \cdots, \transpose{\boldsymbol{\eta}_T}]}$ and data $\mathbf{y} = \transpose{[\transpose{\mathbf{y}}_1, \cdots, \transpose{\mathbf{y}}_T]}$ given parameters: 
    $$
            \mathbf{z} =
            \begin{bmatrix}
                \boldsymbol{\eta} \\ 
                \mathbf{y}
            \end{bmatrix}     
            \sim        
            \mathcal{N}(\mathbf{0}, \mathbf{Q}^{-1})
            ; \: \mathbf{Q} = 
            \begin{bmatrix}
                \mathbf{Q}_{\eta} & \mathbf{Q}_{\eta, y} \\
                \transpose{\mathbf{Q}}_{\eta, y} & \mathbf{Q}_{y}
            \end{bmatrix}   
            \hyperlink{app_mapping}{\beamerbutton{Mapping from $\Theta$ to $\mathbf{Q}$}} 
    $$ \\~\\    
    
    Standard result for the multivariate $\mathcal{N}$: $p(\boldsymbol{\eta} | \mathbf{y}, \Theta) = \mathcal{N}(-\mathbf{Q}_{\eta}^{-1} \mathbf{Q}_{\eta y} \mathbf{y}, \mathbf{Q}_{\eta}^{-1})$\\~\\

    Sampling from this distribution does \textbf{not} require the inversion of (the potentially very large matrix) $\mathbf{Q}_{\eta}$ and because it is banded 
    \vspace{0.2cm}
    \begin{itemize}
        \item the mean $-\mathbf{Q}_{\eta}^{-1} \mathbf{Q}_{\eta y} \mathbf{y}$ 
        \item and a random draw given mean and precision matrix 
    \end{itemize}
    \vspace{0.2cm}
    can be obtained efficiently! \hyperlink{app_rueheld_algs}{\beamerbutton{\citet[][Algorithms 2.1, 2.4]{rueheld_book2005}}}
    
\end{frame}

\begin{frame}{Precision-based sampling with missing observations}{Drawing from $p(\boldsymbol{\eta}, \mathbf{y}^m | \mathbf{y}^o, \Theta)$: 3 block ordering}
    What to do when some observations in $\mathbf{y}$ are missing? \\~\\

    E.g. a model with one factor, two variables and $t=1:6$ with $$\mathbf{y}=\transpose{[y_{11}, y_{12}, \text{NaN}, y_{22}, y_{31}, \text{NaN},  \text{NaN}, \text{NaN}, y_{51}, \text{NaN},  \text{NaN},  \text{NaN}]}$$\\~\\

    Reorder the model in 3 blocks as: 
    $$
    \mathbf{z}_{3b} =
    \begin{bmatrix}
        \boldsymbol{\eta} \\ 
        \mathbf{y^m}\\
        \mathbf{y^o}
    \end{bmatrix}  
    \equiv
    \begin{bmatrix}
        \mathbf{z}^{3b}_{\eta y^m} \\
        \mathbf{z}^{3b}_{y^o}
    \end{bmatrix}     
    \sim        
    \mathcal{N}(\mathbf{0}, \mathbf{Q}_{3b}^{-1})
    ; \: \mathbf{Q}_{3b} 
    = 
    % \begin{bmatrix}
    %     \mathbf{Q}_{\eta} & \mathbf{Q}_{\eta, y^m} & \mathbf{Q}_{\eta y^o} \\
    %     \transpose{\mathbf{Q}}_{\eta, y^m} & \mathbf{Q}_{y^m} & \mathbf{Q}_{y^m, y^o} \\
    %     \transpose{\mathbf{Q}}_{\eta, y^o} & \transpose{\mathbf{Q}}_{y^m, y^o} & \mathbf{Q}_{y^o}
    % \end{bmatrix}   
    % \equiv
    \begin{bmatrix}
        \mathbf{Q}^{3b}_{\eta y^m} & \mathbf{Q}^{3b}_{\eta y^m, y^o}  \\
        \mathbf{Q}^{3b, \sf{T}}_{\eta y^m, y^o} & \mathbf{Q}^{3b}_{ y^o} 
    \end{bmatrix} 
    $$ \\~\\ 

    \textbf{Example}: 
    $$\mathbf{z}_{3b} = \transpose{   
    [\textcolor{myblue}{\boldsymbol{\eta}}, \textcolor{myred}{\mathbf{y^m}}, \textcolor{amber}{\mathbf{y^o}}]
    } = 
    \transpose{[\textcolor{myblue}{\eta_1}, \textcolor{myblue}{\dots}, \textcolor{myblue}{\eta_6}, \textcolor{myred}{y_{21}}, \textcolor{myred}{y_{32}}, \textcolor{myred}{y_{41}}, \textcolor{myred}{y_{42}}, \textcolor{myred}{y_{52}}, \textcolor{myred}{y_{61}}, \textcolor{myred}{y_{62}}, \textcolor{amber}{y_{11}}, \textcolor{amber}{y_{12}}, \textcolor{amber}{y_{22}}, \textcolor{amber}{y_{31}}, \textcolor{amber}{y_{51}}]}
    $$    
\end{frame}

\begin{frame}{Precision-based sampling with missing observations}{Drawing from $p(\boldsymbol{\eta}, \mathbf{y}^m | \mathbf{y}^o, \Theta)$: time-t ordering}

    However, $\mathbf{Q}^{3b}_{\eta y^m}$ will in general \textbf{not} be a banded matrix, so sampling as in the case of complete data not feasible! \\~\\ 
    
    Different ordering of $\mathbf{z}$ that groups conditionally dependent components \textendash\, $\boldsymbol{\eta}_t$  and $\mathbf{y}^m_t$ (\textit{time-t ordering}) \textendash\, together:  $\Longrightarrow$ $\mathbf{z}_{\tau} = \mathcal{P}_{\tau} \mathbf{z}$ and corresponding precision matrix $Q_{\tau} = \mathcal{P}_{\tau} \mathbf{Q} \transpose{\mathcal{P}_{\tau}}$\\~\\ 

    \textbf{Example}: $\mathbf{z}_{\tau} = \transpose{[\textcolor{myblue}{\eta_1}, \textcolor{myblue}{\eta_2},  \textcolor{myred}{y_{21}}, \textcolor{myblue}{\eta_3}, \textcolor{myred}{y_{32}}, \textcolor{myblue}{\eta_4}, \textcolor{myred}{y_{41}}, \textcolor{myred}{y_{42}}, \textcolor{myblue}{\eta_5}, \textcolor{myred}{y_{52}},\textcolor{myblue}{\eta_6}, \textcolor{myred}{y_{61}}, \textcolor{myred}{y_{62}}, \textcolor{amber}{y_{11}} \textcolor{amber}{, \dots, } \textcolor{amber}{y_{51}}]}$ \\~\\  

    This ensures that the precision matrix of the conditional distribution $Q^{\tau}_{\eta y^m}$ is banded!\\~\\  

    After sampling from $\mathbf{z}^{\tau}_{\eta y^m|y^o} \sim \mathcal{N}(-\mathbf{Q}^{\tau -1}_{\eta y^m} \mathbf{Q}^{\tau}_{\eta y^m, y^p} \mathbf{y^o}, \mathbf{Q}^{\tau -1}_{\eta y^m})$, reverse the permutation to back out the draw of $\boldsymbol{\eta}$ and $\mathbf{y}^m$
\end{frame}

\begin{frame}{Precision-based sampling with missing observations}{Example: Graphical comparison of $\mathbf{Q}_{3b}$ and $\mathbf{Q}_{\tau}$}
    \hspace{4.5cm} $\mathbf{Q}_{3b}$ \hspace{3.7cm} $\mathbf{Q}_{\tau}$

    \vspace{0.2cm}
    \begin{adjustbox}{minipage=0.6\textwidth,center}
        \begin{figure}
            \includegraphics[scale = 0.7]{fig_Q3b_Qtau.png}  \vspace{0.1cm} \\
        \end{figure}
            \setstretch{0.1}
            {\tiny \textbf{Notes}: The blue dots indicate the non-zero entries in the precision matrix of the 3-block ordering (left) and the time-t permutation (right) of the discussed example with 2 variables and one factor. The highlighted upper left submatrices correspond to the conditional precision matrix of factors and missing values given observations,$\mathbf{Q}^{3b}_{\eta y^m}$ and $\mathbf{Q}^{\tau}_{\eta y^m}$.}\par
    \end{adjustbox}  
\end{frame}

\begin{frame}{Conditional forecasts in large factor models}{Motivation}
    
    Conditional forecast evaluations in the literature typically condition on realizations (\citealp[][\scriptsize \textbf{JAE}\normalsize]{clarkmccracken_2017_jae}; \citealp[][\scriptsize \textbf{IJoF}\normalsize]{bgl_2015ijf}) $\Longrightarrow$ useful for model assessement and scenario analysis \\~\\  

    Practitioners may also be interested in knowing how accurate forecasts will be when conditioning on external (but quite likely imperfect) information $\Longrightarrow$ compare \textbf{information sets} rather than models \\~\\  

    I condition on professionals' forecasts for GDP and CPI:
    \vspace{0.2cm}
    \begin{itemize}
        \item (conditional) forecasts $\Longrightarrow$  missing values!
        \item similar approach for the Euro area using BVARs: \citet[][\scriptsize \textbf{IJoF}\normalsize]{ganicsodendahl_2021_ijf}
        \item focus of the evaluation on a large cross-section of variables not typically considered in the forecasting literature 
    \end{itemize}    
    
      
\end{frame}

\begin{frame}{Conditional forecasts in large factor models}{Data}

    Quarterly real-time dataset for the German economy (in total \textbf{57 series}):
    {\footnotesize
    \begin{itemize}
        \item activity indicators (expenditure and production components, IP, orders, turnovers)
        \item prices (CPI, PPI, deflators corresponding to chained volume indices from the national accounts)
        \item labor market (employment, wages, hours worked)
        \item financial indicators (interest rates, stock market prices, exchange rates)
        \item survey indicators (sectoral ESI data and employment expectations index)\\~\\
    \end{itemize}
    }

    Reuters Poll of professional forecasters
    {\footnotesize
    \begin{itemize}
      \item $\approx 20$ different forecasts from private sector and research institutes, up to two quarters ahead ($h=2$) 
      \item quarterly GDP growth and y/y CPI inflation (transformed to coincide with model definition of q/q change in CPI)
      \item GDP (inflation) nowcasts, i.e. $h=0$, considerably (slightly) more accurate than model's unconditional forecast
    \end{itemize}
    }
\end{frame}

\begin{frame}{Conditional forecasts in large factor models}{Reuters Poll of professional forecasters}    

    \begin{figure}
        \includegraphics[scale = 0.45]{fig_ReutersPoll.pdf}
    \end{figure}
    \begin{adjustbox}{minipage=0.8\textwidth,center}
            \setstretch{0.1}
            {\tiny \textbf{Notes}: Median forecast from the Reuters Poll of professional forecasters for quarter-on quarter change in the comsumer price index (top panel) and quarter-on-quarter GDP growth. The forecast horizon $h$ is in quarters and relative to the reference period. Source: Thomson Reuters, author's calculations.}\par
    \end{adjustbox} 
\end{frame}


\begin{frame}{Conditional forecasts in large factor models}{Predictive density and forecast set-up}

    Precision-based algorithms to sample from the predictive density: 
    $$p(\mathbf{y^{\sf{f}}}|\mathbf{y^{\sf{o}}}, \mathbf{y^{\sf{c}}}) \propto \int_{\Theta} p(\mathbf{y^{\sf{f}}}|\mathbf{y^{\sf{o}}},\mathbf{y^{\sf{c}}}, \Theta) p(\Theta|\mathbf{y^{\sf{o}}}, \mathbf{y^{\sf{c}}})\, \text{d}\Theta$$ \\~\\

    Estimation sample starts in 1996Q1, factor model with $R=2$, evaluation sample: 2006Q1-2017Q4 \\~\\

    Point (RMSFE) and density forecast (CRPS) accuracy, relative to na\"ive benchmark \\~\\

    Real-time evaluation $\Longrightarrow$ exactly mimic the information set available to professional forecasters \\~\\

    Diebold-Mariano tests to assess if differences between unconditional and conditional forecast accuracy is significant
\end{frame}

\begin{frame}{Conditional forecasts in large factor models}{Results: point forecasts}
    \begin{figure}
        \includegraphics[scale = 0.4]{fig_eval_rmsfe.pdf}  \vspace{0.1cm} \\
    \end{figure}

    \begin{adjustbox}{minipage=0.8\textwidth,center}
        \setstretch{0.1}
        {\tiny \textbf{Notes}: Root mean squared forecast error (RMSFE) corresponding to unconditional forecasts (x-axis) and forecasts conditional on professional forecasters’ view on GDP growth and CPI inflation (y-axis) for different time series. Filled points correspond to those variables for which the null hypothesis of the Diebold-Mariano test can be rejected at the 5 percent level.}\par
\end{adjustbox} 
\end{frame}


\begin{frame}{Conditional forecasts in large factor models}{Results: density forecasts}
    \begin{figure}
        \includegraphics[scale = 0.4]{fig_eval_crps.pdf}  \vspace{0.1cm} \\
    \end{figure}

    \begin{adjustbox}{minipage=0.8\textwidth,center}
        \setstretch{0.1}
        {\tiny \textbf{Notes}: Average continuous ranked probability score (CRPS) corresponding to unconditional forecasts (x-axis) and forecasts conditional on professional forecasters’ view on GDP growth and CPI inflation (y-axis) for different time series. Filled points correspond to those variables for which the null hypothesis of the Diebold-Mariano test can be rejected at the 5 percent level.}\par
\end{adjustbox} 
\end{frame}



\begin{frame}{Conditional forecasts in large factor models}{Robustness checks}
    \begin{figure}
        \includegraphics[scale = 0.40]{fig_eval_robustness_Nr.pdf}  \vspace{0.1cm} \\
    \end{figure}

    \begin{adjustbox}{minipage=0.75\textwidth,center}
        \setstretch{0.1}
        {\tiny \textbf{Notes}: Root mean squared forecast error (RMSFE) relative to the autoregressive benchmarks for different number of factors. Entries above (below) the 45-degree line indicate that the model with two factors performs better (worse) than the alternative models.}\par
\end{adjustbox}  
\end{frame}

\begin{frame}[t]{References}
    \scriptsize     
    \bibliographystyle{aer}
    \bibliography{biblio}
\end{frame}

\begin{frame}{Appendix}{Mapping from $\Theta$ to $\mathbf{Q}$}
\label{app_mapping}
Stacked model: $
    \mathbf{z}
    = 
    \begin{bmatrix}
        \boldsymbol{\eta} \\
        \mathbf{y}
    \end{bmatrix} 
    = 
    \begin{bmatrix}
        I & 0 \\
        \boldsymbol{\Lambda} & I 
    \end{bmatrix}
    \begin{bmatrix}
        \boldsymbol{\eta} \\
        \mathbf{e}\\
    \end{bmatrix}
$ where $\boldsymbol{\Lambda} = I_T \otimes \boldsymbol{\lambda}$

Also, let $\mathbf{V}_{\!e} = I_T \otimes \Sigma_e = \mathbf{S}_{e}^{-1}$, $\mathbf{V}_{\!u} = I_T \otimes \Sigma_u = \mathbf{S}_{u}^{-1}$ and $
\mathbf{H} = \begin{bmatrix}
        1 &  &  &   \\
        -\phi & 1 &  &  \\
         & \ddots & \ddots &  \\
         &  & -\phi & 1
    \end{bmatrix}
$

Then 
\begin{align*}
    \mathbf{Q}_z 
    &= 
    \text{\sf{Var}}\left(
    \begin{bmatrix}
        \boldsymbol{\eta} \\
        \mathbf{y}
    \end{bmatrix}
    \right)^{-1} 
    =  
    \left(
    \begin{bmatrix}
        I & 0 \\
        \boldsymbol{\Lambda} & I
    \end{bmatrix}
    \begin{bmatrix}
        \mathbf{H}^{-1} \mathbf{V}_{\!u} \transpose{\mathbf{H}^{-1}} & 0 \\
        0 & \mathbf{V}_{\!e}
    \end{bmatrix} 
    \begin{bmatrix}
        I & \transpose{\boldsymbol{\Lambda}} \\
        0 & I
    \end{bmatrix}
    \right)^{-1}\\
    &=
    \begin{bmatrix}
        I & -\transpose{\boldsymbol{\Lambda}} \\
        0 & I
    \end{bmatrix}
    \begin{bmatrix}
        \transpose{\mathbf{H}} \mathbf{S}_{u} \mathbf{H} & 0 \\
        0 & \mathbf{S}_{e}
    \end{bmatrix} 
    \begin{bmatrix}
        I & 0 \\
        -\boldsymbol{\Lambda} & I 
    \end{bmatrix} \\
    &=
    \begin{bmatrix}
         \transpose{\mathbf{H}} \mathbf{S}_{u} \mathbf{H} + \transpose{\boldsymbol{\Lambda}}\mathbf{V}_{e}\boldsymbol{\Lambda}  & -\transpose{\boldsymbol{\Lambda}} \mathbf{S}_{e} \\
         -\mathbf{S}_{e} \boldsymbol{\Lambda} & \mathbf{S}_{e}
    \end{bmatrix}
    \equiv 
    \begin{bmatrix}
        \mathbf{Q}_{\eta} & \mathbf{Q}_{\eta y} \\
        \transpose{\mathbf{Q}}_{\eta y} & \mathbf{Q}_{y}  
    \end{bmatrix}
\end{align*}

\end{frame}

\begin{frame}{Appendix}{\citet[][Algorithm 2.1, 2.4]{rueheld_book2005}}
    \label{app_rueheld_algs}
    \setcounter{algorithm}{0}
    \begin{algorithm}[H]
        \caption{Solving $\mathbf{A} \mathbf{x} = \mathbf{b}$ where $\mathbf{A} > 0$}
        \begin{algorithmic}[1]
            \State Compute the Cholesky factorization $\mathbf{A} = \mathbf{L}\transpose{\mathbf{L}}$
            \State Solve $\mathbf{L}\mathbf{v} = \mathbf{b}$ via forward substitution
            \State Solve $\transpose{\mathbf{L}}\mathbf{x} = \mathbf{v}$ via backward substitution\\
            \Return $\mathbf{x}$
        \end{algorithmic}
    \end{algorithm}
    \setcounter{algorithm}{3}
    \begin{algorithm}[H]
        \caption{Sampling $\mathbf{x} \sim \mathcal{N}(\boldsymbol{\mu}, \mathbf{Q}^{-1})$}
        \begin{algorithmic}[1]
            \State Compute the Cholesky factorization $\mathbf{Q} = \mathbf{L}\transpose{\mathbf{L}}$
            \State Sample $\mathbf{z} \sim \mathcal{N}(\boldsymbol{\mu}, \mathbf{I})$
            \State Solve $\transpose{\mathbf{L}} \mathbf{v} = \mathbf{z}$
            \State Compute $\mathbf{x} = \boldsymbol{\mu} + \mathbf{v}$\\
            \Return $\mathbf{x}$
        \end{algorithmic}
    \end{algorithm}

    \hyperlink{precsampl_draw_eta}{\beamerreturnbutton{Back}}
\end{frame}

\end{document}